{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtfktE013ONV"
      },
      "source": [
        "# Video Vision Transformer\n",
        "\n",
        "**Author:** [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Ayush Thakur](https://twitter.com/ayushthakur0) (equal contribution)<br>\n",
        "**Date created:** 2022/01/12<br>\n",
        "**Last modified:**  2024/01/15<br>\n",
        "**Description:** A Transformer-based architecture for video classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZIvkRIi3ONW"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Videos are sequences of images. Let's assume you have an image\n",
        "representation model (CNN, ViT, etc.) and a sequence model\n",
        "(RNN, LSTM, etc.) at hand. We ask you to tweak the model for video\n",
        "classification. The simplest approach would be to apply the image\n",
        "model to individual frames, use the sequence model to learn\n",
        "sequences of image features, then apply a classification head on\n",
        "the learned sequence representation.\n",
        "The Keras example\n",
        "[Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/)\n",
        "explains this approach in detail. Alernatively, you can also\n",
        "build a hybrid Transformer-based model for video classification as shown in the Keras example\n",
        "[Video Classification with Transformers](https://keras.io/examples/vision/video_transformers/).\n",
        "\n",
        "In this example, we minimally implement\n",
        "[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)\n",
        "by Arnab et al., a **pure Transformer-based** model\n",
        "for video classification. The authors propose a novel embedding scheme\n",
        "and a number of Transformer variants to model video clips. We implement\n",
        "the embedding scheme and one of the variants of the Transformer\n",
        "architecture, for simplicity.\n",
        "\n",
        "This example requires  `medmnist` package, which can be installed\n",
        "by running the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "u8by_3bv3ONX"
      },
      "outputs": [],
      "source": [
        "!pip install -qq medmnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4D-6IOA3ONX"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Wa-b2eg13ONY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import imageio\n",
        "import medmnist\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf  # for data preprocessing only\n",
        "import keras\n",
        "from keras import layers, ops\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "keras.utils.set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uEe8rMT3ONY"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "The hyperparameters are chosen via hyperparameter\n",
        "search. You can learn more about the process in the \"conclusion\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fWnqzH8W3ONZ"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "DATASET_NAME = \"tuyen_behaviors\"\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (15, 128, 128, 3)\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 100\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylGKtaxB3ONZ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "For our example we use the\n",
        "[MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification](https://medmnist.com/)\n",
        "dataset. The videos are lightweight and easy to train on."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fo9Lw1FJ3lld",
        "outputId": "9a3d8fc3-9800-4df2-cd23-54c8373b3e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Dataset_CH/Tuyen_Behaviors/Data_Thucnghiem/dataset_final_goc\n",
        "\n",
        "# Load the saved data\n",
        "X_images = np.load(\"X_image_train.npy\")\n",
        "y = np.load(\"y_train.npy\")\n",
        "X_test_images = np.load(\"X_image_test.npy\")\n",
        "y_test = np.load(\"y_test.npy\")\n",
        "\n",
        "\n",
        "# Check the shape of the loaded arrays to verify successful loading\n",
        "print(\"X_images shape:\", X_images.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"X_test_images shape:\", X_test_images.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "WHbee0fm3qTY",
        "outputId": "e13f27b9-f814-4c92-a8f7-5bd1ceb57d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/12c07MAQ1tyOViK3Ep1gkSa6ctNBB8KIa/Data_Thucnghiem/dataset_final_goc\n",
            "X_images shape: (1082, 15, 128, 128, 3)\n",
            "y shape: (1082,)\n",
            "X_test_images shape: (272, 15, 128, 128, 3)\n",
            "y_test shape: (272,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(y, num_classes=4)"
      ],
      "metadata": {
        "id": "bJLNB3yP3_gA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_images_train, X_images_valid, y_train, y_valid = train_test_split(X_images, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9BVtlsv64BaB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve4gYw483ONZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def download_and_prepare_dataset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "        # Get videos\n",
        "        train_videos = data[\"train_images\"]\n",
        "        valid_videos = data[\"val_images\"]\n",
        "        test_videos = data[\"test_images\"]\n",
        "\n",
        "        # Get labels\n",
        "        train_labels = data[\"train_labels\"].flatten()\n",
        "        valid_labels = data[\"val_labels\"].flatten()\n",
        "        test_labels = data[\"test_labels\"].flatten()\n",
        "\n",
        "    return (\n",
        "        (train_videos, train_labels),\n",
        "        (valid_videos, valid_labels),\n",
        "        (test_videos, test_labels),\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[DATASET_NAME]\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = download_and_prepare_dataset(info)\n",
        "(train_videos, train_labels) = prepared_dataset[0]\n",
        "(valid_videos, valid_labels) = prepared_dataset[1]\n",
        "(test_videos, test_labels) = prepared_dataset[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCbQKL_r3ONa"
      },
      "source": [
        "### `tf.data` pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4Lne25gn3ONa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
        "    # Preprocess images\n",
        "    frames = tf.image.convert_image_dtype(\n",
        "        frames[\n",
        "            ..., tf.newaxis\n",
        "        ],  # The new axis is to help for further processing with Conv3D layers\n",
        "        tf.float32,\n",
        "    )\n",
        "    # Parse label\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "\n",
        "    if loader_type == \"train\":\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "\n",
        "    dataloader = (\n",
        "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "trainloader = prepare_dataloader(X_images_train, y_train, \"train\")\n",
        "validloader = prepare_dataloader(X_images_valid, y_valid, \"valid\")\n",
        "testloader = prepare_dataloader(X_test_images, y_test, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGMNgvcj3ONa"
      },
      "source": [
        "## Tubelet Embedding\n",
        "\n",
        "In ViTs, an image is divided into patches, which are then spatially\n",
        "flattened, a process known as tokenization. For a video, one can\n",
        "repeat this process for individual frames. **Uniform frame sampling**\n",
        "as suggested by the authors is a tokenization scheme in which we\n",
        "sample frames from the video clip and perform simple ViT tokenization.\n",
        "\n",
        "| ![uniform frame sampling](https://i.imgur.com/aaPyLPX.png) |\n",
        "| :--: |\n",
        "| Uniform Frame Sampling [Source](https://arxiv.org/abs/2103.15691) |\n",
        "\n",
        "**Tubelet Embedding** is different in terms of capturing temporal\n",
        "information from the video.\n",
        "First, we extract volumes from the video -- these volumes contain\n",
        "patches of the frame and the temporal information as well. The volumes\n",
        "are then flattened to build video tokens.\n",
        "\n",
        "| ![tubelet embedding](https://i.imgur.com/9G7QTfV.png) |\n",
        "| :--: |\n",
        "| Tubelet Embedding [Source](https://arxiv.org/abs/2103.15691) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ELa0V2XN3ONb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v70gKYp3ONb"
      },
      "source": [
        "## Positional Embedding\n",
        "\n",
        "This layer adds positional information to the encoded video tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3AD1DemW3ONb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = ops.arange(0, num_tokens, 1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_iYgo1R3ONb"
      },
      "source": [
        "## Video Vision Transformer\n",
        "\n",
        "The authors suggest 4 variants of Vision Transformer:\n",
        "\n",
        "- Spatio-temporal attention\n",
        "- Factorized encoder\n",
        "- Factorized self-attention\n",
        "- Factorized dot-product attention\n",
        "\n",
        "In this example, we will implement the **Spatio-temporal attention**\n",
        "model for simplicity. The following code snippet is heavily inspired from\n",
        "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n",
        "One can also refer to the\n",
        "[official repository of ViViT](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)\n",
        "which contains all the variants, implemented in JAX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "45o4kQZ63ONc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=ops.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=ops.gelu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36XeGnO3ONc"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTW93qcJ3ONc",
        "outputId": "7084aca1-3475-403e-f321-7fecbb1899fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 999ms/step - accuracy: 0.2262 - loss: 1.7261 - top-5-accuracy: 1.0000 - val_accuracy: 0.2074 - val_loss: 1.4427 - val_top-5-accuracy: 1.0000\n",
            "Epoch 2/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.2533 - loss: 1.4195 - top-5-accuracy: 1.0000 - val_accuracy: 0.2765 - val_loss: 1.3711 - val_top-5-accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.2951 - loss: 1.3996 - top-5-accuracy: 1.0000 - val_accuracy: 0.2535 - val_loss: 1.5416 - val_top-5-accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 136ms/step - accuracy: 0.2773 - loss: 1.4541 - top-5-accuracy: 1.0000 - val_accuracy: 0.2811 - val_loss: 1.3755 - val_top-5-accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - accuracy: 0.3081 - loss: 1.3810 - top-5-accuracy: 1.0000 - val_accuracy: 0.2811 - val_loss: 1.3865 - val_top-5-accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.2982 - loss: 1.3920 - top-5-accuracy: 1.0000 - val_accuracy: 0.2581 - val_loss: 1.4533 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - accuracy: 0.2874 - loss: 1.4327 - top-5-accuracy: 1.0000 - val_accuracy: 0.3502 - val_loss: 1.3501 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - accuracy: 0.2997 - loss: 1.3794 - top-5-accuracy: 1.0000 - val_accuracy: 0.3272 - val_loss: 1.3405 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.3326 - loss: 1.3582 - top-5-accuracy: 1.0000 - val_accuracy: 0.2581 - val_loss: 1.4202 - val_top-5-accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 144ms/step - accuracy: 0.3103 - loss: 1.3993 - top-5-accuracy: 1.0000 - val_accuracy: 0.3594 - val_loss: 1.3341 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.3529 - loss: 1.3187 - top-5-accuracy: 1.0000 - val_accuracy: 0.3548 - val_loss: 1.3300 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - accuracy: 0.3583 - loss: 1.3032 - top-5-accuracy: 1.0000 - val_accuracy: 0.3041 - val_loss: 1.4543 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - accuracy: 0.3818 - loss: 1.3532 - top-5-accuracy: 1.0000 - val_accuracy: 0.3041 - val_loss: 1.3683 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - accuracy: 0.4037 - loss: 1.2576 - top-5-accuracy: 1.0000 - val_accuracy: 0.3548 - val_loss: 1.2675 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def run_experiment():\n",
        "    # Initialize model\n",
        "    model = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "        ),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
        "    )\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxK3Cvro3ONc"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnZCKJen3ONc"
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES_VIZ = 25\n",
        "testsamples, labels = next(iter(testloader))\n",
        "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
        "\n",
        "ground_truths = []\n",
        "preds = []\n",
        "videos = []\n",
        "\n",
        "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
        "    # Generate gif\n",
        "    testsample = np.reshape(testsample.numpy(), (-1, 28, 28))\n",
        "    with io.BytesIO() as gif:\n",
        "        imageio.mimsave(gif, (testsample * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
        "        videos.append(gif.getvalue())\n",
        "\n",
        "    # Get model prediction\n",
        "    output = model.predict(ops.expand_dims(testsample, axis=0))[0]\n",
        "    pred = np.argmax(output, axis=0)\n",
        "\n",
        "    ground_truths.append(label.numpy().astype(\"int\"))\n",
        "    preds.append(pred)\n",
        "\n",
        "\n",
        "def make_box_for_grid(image_widget, fit):\n",
        "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
        "\n",
        "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
        "    \"\"\"\n",
        "    # Make the caption\n",
        "    if fit is not None:\n",
        "        fit_str = \"'{}'\".format(fit)\n",
        "    else:\n",
        "        fit_str = str(fit)\n",
        "\n",
        "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
        "\n",
        "    # Make the green box with the image widget inside it\n",
        "    boxb = ipywidgets.widgets.Box()\n",
        "    boxb.children = [image_widget]\n",
        "\n",
        "    # Compose into a vertical box\n",
        "    vb = ipywidgets.widgets.VBox()\n",
        "    vb.layout.align_items = \"center\"\n",
        "    vb.children = [h, boxb]\n",
        "    return vb\n",
        "\n",
        "\n",
        "boxes = []\n",
        "for i in range(NUM_SAMPLES_VIZ):\n",
        "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
        "    true_class = info[\"label\"][str(ground_truths[i])]\n",
        "    pred_class = info[\"label\"][str(preds[i])]\n",
        "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
        "\n",
        "    boxes.append(make_box_for_grid(ib, caption))\n",
        "\n",
        "ipywidgets.widgets.GridBox(\n",
        "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEHaQRq-3ONc"
      },
      "source": [
        "## Final thoughts\n",
        "\n",
        "With a vanilla implementation, we achieve ~79-80% Top-1 accuracy on the\n",
        "test dataset.\n",
        "\n",
        "The hyperparameters used in this tutorial were finalized by running a\n",
        "hyperparameter search using\n",
        "[W&B Sweeps](https://docs.wandb.ai/guides/sweeps).\n",
        "You can find out our sweeps result\n",
        "[here](https://wandb.ai/minimal-implementations/vivit/sweeps/66fp0lhz)\n",
        "and our quick analysis of the results\n",
        "[here](https://wandb.ai/minimal-implementations/vivit/reports/Hyperparameter-Tuning-Analysis--VmlldzoxNDEwNzcx).\n",
        "\n",
        "For further improvement, you could look into the following:\n",
        "\n",
        "- Using data augmentation for videos.\n",
        "- Using a better regularization scheme for training.\n",
        "- Apply different variants of the transformer model as in the paper.\n",
        "\n",
        "We would like to thank [Anurag Arnab](https://anuragarnab.github.io/)\n",
        "(first author of ViViT) for helpful discussion. We are grateful to\n",
        "[Weights and Biases](https://wandb.ai/site) program for helping with\n",
        "GPU credits.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/video-vision-transformer)\n",
        "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/video-vision-transformer-CT)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vivit",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}